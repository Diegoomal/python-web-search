{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results=[WebSearchResult(content='[Cloud models](https://ollama.com/blog/cloud-models) are now available in Ollama\\n\\n**Chat & build with open models**\\n\\n[Download](https://ollama.com/download) [Explore models](https://ollama.com/models)\\n\\nAvailable for macOS, Windows, and Linux', title='Ollama', url='https://ollama.com/'), WebSearchResult(content=\"_keyboard\\\\_arrow\\\\_down_ Categories\\n\\n- All Tutorials\\n- [WordPress](https://www.hostinger.com/tutorials/wordpress)\\n- [VPS](https://www.hostinger.com/tutorials/vps)\\n- [Website development](https://www.hostinger.com/tutorials/website-development)\\n- [Ecommerce](https://www.hostinger.com/tutorials/ecommerce)\\n- [Hostinger Horizons](https://www.hostinger.com/tutorials/hostinger-horizons)\\n- [How to make a website](https://www.hostinger.com/tutorials/how-to-make-a-website)\\n\\n_search_\\n\\n_close_\\n\\n[VPS](https://www.hostinger.com/tutorials/vps) [VPS for web devs](https://www.hostinger.com/tutorials/vps/vps-for-web-devs)\\n\\nMay 08, 2025\\n\\nAriffud M.\\n\\n6min Read\\n\\n# What is Ollama? Understanding how it works, main features and models\\n\\n[Copy link\\\\\\n\\\\\\nCopied!](https://www.hostinger.com/www.hostinger.com)\\n\\nOllama is an open-source tool that allows you to run large language models (LLMs) directly on your local machine. This makes it ideal for AI developers, researchers, and businesses prioritizing data control and privacy.\\n\\nBy running models locally, you maintain full data ownership and avoid the potential security risks associated with cloud storage. Offline AI tools like Ollama also help reduce latency and reliance on external servers, making them faster and more reliable.\\n\\nThis article will explore Ollama’s key features, supported models, and practical use cases. By the end, you’ll be able to determine if this LLM tool suits your AI-based projects and needs.\\n\\n[Download ChatGPT cheat sheet](https://assets.hostinger.com/content/tutorials/pdf/ChatGPT-Cheat-Sheet.pdf)\\n\\n## How Ollama works\\n\\nOllama creates an isolated environment to run LLMs locally on your system, which prevents any potential conflicts with other installed software. This environment already includes all the necessary components for deploying AI models, such as:\\n\\n- **Model weights**. The pre-trained data that the model uses to function.\\n- **Configuration files**. Settings that define how the model behaves.\\n- **Necessary dependencies**. Libraries and tools that support the model’s execution.\\n\\nTo put it simply, first – you pull models from the Ollama library. Then, you run these models as-is or adjust parameters to customize them for specific tasks. After the setup, you can interact with the models by entering prompts, and they’ll generate the responses.\\n\\nThis advanced AI tool works best on [discrete graphical processing unit (GPU) systems](https://www.intel.com/content/www/us/en/support/articles/000057824/graphics.html). While you can run it on CPU-integrated GPUs, using dedicated compatible GPUs instead, like those from NVIDIA or AMD, will reduce processing times and ensure smoother AI interactions.\\n\\nWe recommend checking [Ollama’s official GitHub page](https://github.com/ollama/ollama/blob/main/docs/gpu.md) for GPU compatibility.\\n\\n## Key features of Ollama\\n\\nOllama offers several key features that make offline model management easier and enhance performance.\\n\\n### Local AI model management\\n\\nOllama grants you full control to download, update, and delete models easily on your system. This feature is valuable for developers and researchers who prioritize strict data security.\\n\\nIn addition to basic management, Ollama lets you track and control different model versions. This is essential in research and production environments, where you might need to revert to or test multiple model versions to see which generates the desired results.\\n\\n### Command-line and GUI options\\n\\nOllama mainly operates through a [command-line interface (CLI)](https://www.hostinger.com/tutorials/what-is-cli), giving you precise control over the models. The CLI allows for quick commands to pull, run, and manage models, which is ideal if you’re comfortable working in a terminal window.\\n\\nIf you’re interested in a command-line approach, feel free to check out our [Ollama CLI tutorial](https://www.hostinger.com/tutorials/ollama-cli-tutorial).\\n\\nOllama also supports third-party graphical user interface (GUI) tools, such as [Open WebUI](https://openwebui.com), for those who prefer a more visual approach.\\n\\nYou can learn more about using a graphical interface in our [Ollama GUI guide.](https://www.hostinger.com/tutorials/ollama-gui-tutorial)\\n\\n### Multi-platform support\\n\\nAnother standout feature of Ollama is its broad support for various platforms, including macOS, Linux, and Windows.\\n\\nThis cross-platform compatibility ensures you can easily integrate Ollama into your existing workflows, regardless of your preferred operating system. However, note that Windows support is currently in preview.\\n\\nAdditionally, Ollama’s compatibility with Linux lets you [install it on a virtual private server (VPS)](https://www.hostinger.com/tutorials/how-to-install-ollama). Compared to running Ollama on local machines, using a VPS lets you access and manage models remotely, which is ideal for larger-scale projects or team collaboration.\\n\\n## Available models on Ollama\\n\\nOllama supports numerous ready-to-use and customizable [large language models](https://www.hostinger.com/tutorials/large-language-models) to meet your project’s specific requirements. Here are some of the most popular Ollama models:\\n\\n[Llama 3.2](https://ollama.com/library/llama3.2)\\n\\nLlama 3.2 is a versatile model for natural language processing (NLP) tasks, like text generation, summarization, and machine translation. Its ability to understand and generate human-like text makes it popular for developing chatbots, writing content, and building conversational AI systems.\\n\\nYou can fine-tune Llama 3.2 for specific industries and niche applications, such as customer service or product recommendations. With solid multilingual support, this model is also favored for building machine translation systems that are useful for global companies and multinational environments.\\n\\n[Mistral](https://ollama.com/library/mistral)\\n\\nMistral handles code generation and large-scale data analysis, making it ideal for developers working on AI-driven coding platforms. Its pattern recognition capabilities enable it to tackle complex programming tasks, automate repetitive coding processes, and identify bugs.\\n\\nSoftware developers and researchers can customize Mistral to generate code for different programming languages. Additionally, its data processing ability makes it useful for managing large datasets in the finance, healthcare, and eCommerce sectors.\\n\\n[Code Llama](https://ollama.com/library/codellama)\\n\\nAs the name suggests, Code Llama excels at programming-related tasks, such as writing and reviewing code. It automates coding workflows to boost productivity for software developers and engineers.\\n\\nCode Llama integrates well with existing development environments, and you can tweak it to understand different coding styles or programming languages. As a result, it can handle more complex projects, such as API development and system optimization.\\n\\n[LLaVA](https://ollama.com/library/llava)\\n\\nLLaVA is a [multimodal model](https://cloud.google.com/use-cases/multimodal-ai) capable of processing text and images, which is perfect for tasks that require visual data interpretation. It’s primarily used to generate accurate image captions, answer visual questions, and enhance user experiences through combined text and image analysis.\\n\\nIndustries like eCommerce and digital marketing benefit from LLaVA to analyze product images and generate relevant content. Researchers can also adjust the model to interpret medical images, such as X-rays and MRIs.\\n\\n[Phi-3](https://ollama.com/library/phi3)\\n\\nPhi-3 is designed for scientific and research-based applications. Its training on extensive academic and research datasets makes it particularly useful for tasks like literature reviews, data summarization, and scientific analysis.\\n\\nMedicine, biology, and environmental science researchers can fine-tune Phi-3 to quickly analyze and interpret large volumes of scientific literature, extract key insights, or summarize complex data.\\n\\nIf you’re unsure which model to use, you can explore [Ollama’s model library](https://ollama.com/library), which provides detailed information about each model, including installation instructions, supported use cases, and customization options.\\n\\n#### Suggested reading\\n\\nFor the best results when building advanced AI applications, consider combining LLMs with [generative AI](https://www.hostinger.com/tutorials/generative-ai) techniques. Learn more about it in our article.\\n\\n## Use cases for Ollama\\n\\nHere are some examples of how Ollama can impact workflows and create innovative solutions.\\n\\n**Creating local chatbots**\\n\\nWith Ollama, developers can create highly responsive AI-driven chatbots that run entirely on local servers, ensuring that customer interactions remain private.\\n\\nRunning chatbots locally lets businesses avoid the latency associated with cloud-based AI solutions, improving response times for end users. Industries like transportation and education can also fine-tune models to fit specific language or industry jargon.\\n\\n**Conducting local research**\\n\\nUniversities and data scientists can leverage Ollama to conduct offline machine-learning research. This lets them experiment with datasets in privacy-sensitive environments, ensuring the work remains secure and is not exposed to external parties.\\n\\nOllama’s ability to run LLMs locally is also helpful in areas with limited or no internet access. Additionally, research teams can adapt models to analyze and summarize scientific literature or draw out important findings.\\n\\n**Building privacy-focused AI applications**\\n\\nOllama provides an ideal solution for developing privacy-focused AI applications that are ideal for businesses handling sensitive information. For instance, legal firms can create software for contract analysis or legal research without compromising client information.\\n\\nRunning AI locally guarantees that all computations occur within the company’s infrastructure, helping businesses meet regulatory requirements for data protection, such as GDPR compliance, which mandates strict control over data handling.\\n\\n**Integrating AI into existing platforms**\\n\\nOllama can easily integrate with existing software platforms, enabling businesses to include AI capabilities without overhauling their current systems.\\n\\nFor instance, companies using content management systems (CMSs) can integrate local models to improve content recommendations, automate editing processes, or suggest personalized content to engage users.\\n\\nAnother example is integrating Ollama into customer relationship management (CRM) systems to enhance automation and data analysis, ultimately improving decision-making and customer insights.\\n\\n#### Suggested reading\\n\\nDid you know that you can [create your own AI application, like ChatGPT](https://www.hostinger.com/tutorials/how-to-deploy-chatgpt-clone), using OpenAI API? Learn how to do so in our article.\\n\\n## Benefits of using Ollama\\n\\nOllama provides several advantages over cloud-based AI solutions, particularly for users prioritizing privacy and cost efficiency:\\n\\n- **Enhanced privacy and data security**. Ollama keeps sensitive data on local machines, reducing the risk of exposure through third-party cloud providers. This is crucial for industries like legal firms, healthcare organizations, and financial institutions, where data privacy is a top priority.\\n- **No reliance on cloud services**. Businesses maintain complete control over their infrastructure without relying on external cloud providers. This independence allows for greater scalability on local servers and ensures that all data remains within the organization’s control.\\n- **Customization flexibility**. Ollama lets developers and researchers tweak models according to specific project requirements. This flexibility ensures better performance on tailored datasets, making it ideal for research or niche applications where a one-size-fits-all cloud solution may not be suitable.\\n- **Offline access**. Running AI models locally means you can work without internet access. This is especially useful in environments with limited connectivity or for projects requiring strict control over data flow.\\n- **Cost savings**. By eliminating the need for cloud infrastructure, you avoid recurring costs related to cloud storage, data transfer, and usage fees. While cloud infrastructure may be convenient, running models offline can lead to significant long-term savings, particularly for projects with consistent, heavy usage.\\n\\n## Conclusion\\n\\nOllama is ideal for developers and businesses looking for a flexible, privacy-focused AI solution. It lets you run LLMs locally and provides complete control over data privacy and security.\\n\\nAdditionally, Ollama’s ability to adjust models makes it a powerful option for specialized projects. Whether you’re developing chatbots, conducting research, or building privacy-centric applications, it offers a cost-effective alternative to cloud-based AI solutions.\\n\\nFinally, if you’re looking for a tool that offers both control and customization for your AI-based projects, Ollama is definitely worth exploring.\\n\\n## What is Ollama FAQ\\n\\n### What is Ollama AI used for?\\n\\nOllama runs and manages large language models (LLMs) locally on your machine. It’s ideal for users who want to avoid cloud dependencies, ensuring complete control over data privacy and security while maintaining flexibility in AI model deployment.\\n\\n### Can I customize the AI models in Ollama?\\n\\nYes, you can customize AI models in Ollama using the [Modelfile system](https://github.com/ollama/ollama/blob/main/docs/modelfile.md). This system lets you modify models to fit specific project needs, adjust parameters, or even create new versions based on existing ones.\\n\\n### Is Ollama better than ChatGPT?\\n\\nOllama offers a privacy-focused alternative to ChatGPT by running models and storing data on your system. While ChatGPT provides more scalability through cloud-based infrastructure, it may raise concerns about data security. The better choice depends on your project’s privacy and scalability needs.\\n\\n**All of the tutorial content on this website is subject to [Hostinger's rigorous editorial standards and values.](https://www.hostinger.com/tutorials/editorial-standards-and-values)**\\n\\nThe author\\n\\nAriffud Muhammad\\n\\nAriffud is a Technical Content Writer with an educational background in Informatics. He has extensive expertise in Linux and VPS, authoring over 200 articles on server management and web development. Follow him on [LinkedIn](https://www.linkedin.com/in/ariffud).\\n\\n[More from Ariffud Muhammad](https://www.hostinger.com/tutorials/author/muhammadariffudin)\\n\\n[Copy link\\\\\\n\\\\\\nCopied!](https://www.hostinger.com/www.hostinger.com)\\n\\n## Related tutorials\\n\\n10 Sep •\\n[VPS](https://www.hostinger.com/tutorials/vps)• [Automation](https://www.hostinger.com/tutorials/vps/automation)•\\n\\n##### [How to automate WordPress with n8n](https://www.hostinger.com/tutorials/how-to-integrate-n8n-with-wordpress)\\n\\nWordPress provides a graphical admin dashboard that lets you build, deploy, and manage websites easily without coding. While already efficient by...\\n\\n[By Aris Sentika](https://www.hostinger.com/tutorials/author/aris)\\n\\n10 Sep •\\n[VPS](https://www.hostinger.com/tutorials/vps)• [Automation](https://www.hostinger.com/tutorials/vps/automation)•\\n\\n##### [How to use the n8n WordPress node to create posts with AI](https://www.hostinger.com/tutorials/n8n-wordpress-node-create-post)\\n\\nn8n is a low-code platform that enables you to create automation workflows for various tasks by connecting various tools or applications, including...\\n\\n[By Aris Sentika](https://www.hostinger.com/tutorials/author/aris)\\n\\n21 Aug •\\n[VPS](https://www.hostinger.com/tutorials/vps)• [Automation](https://www.hostinger.com/tutorials/vps/automation)•\\n\\n##### [How to integrate WhatsApp with n8n?](https://www.hostinger.com/tutorials/how-to-integrate-n8n-with-whatsapp)\\n\\nIntegrating n8n with WhatsApp enables you to create an automation workflow for various tasks, including creating a chatbot that responds to user...\\n\\n[By Aris Sentika](https://www.hostinger.com/tutorials/author/aris)\\n\\n## What our customers say\\n\\n[Trustpilot](https://www.trustpilot.com/review/hostinger.com)\\n\\n### Leave a reply [Cancel reply](https://www.hostinger.com/tutorials/what-is-ollama\\\\#respond)\\n\\nPlease fill the required fields.Please accept the privacy checkbox.Please fill the required fields and accept the privacy checkbox.\\n\\nBy using this form you agree that your personal data would be processed in accordance with our [Privacy Policy](https://www.hostinger.com/legal/privacy-policy%20).\\n\\nΔ\\n\\nThank you! Your comment has been successfully submitted. It will be approved within the next 24 hours.\", title='What is Ollama? Introduction to the AI model management tool', url='https://www.hostinger.com/tutorials/what-is-ollama'), WebSearchResult(content='[Join our Discord Server](https://discord.gg/QEkCXAXYSe)\\n\\n[Tanvir Kour Follow](https://collabnix.com/author/tanvirkour/) Tanvir Kour is a passionate technical blogger and open source enthusiast. She is a graduate in Computer Science and Engineering and has 4 years of experience in providing IT solutions. She is well-versed with Linux, Docker and Cloud-Native application. You can connect to her via Twitter https://x.com/tanvirkour\\n\\n# Complete Ollama Guide: Installation, Usage & Code Examples\\n\\n4th August 2025 4 min read\\n\\nTable of Contents[Toggle](https://collabnix.com/collabnix.com)\\n\\n## What is Ollama?\\n\\n[Ollama i](https://ollama.com) s a lightweight, extensible framework for building and running large language models locally. Run LLaMA, Mistral, CodeLlama, and other models on your machine without cloud dependencies.\\n\\n## Quick Installation\\n\\n### macOS\\n\\n```\\n# Download and install\\ncurl -fsSL https://ollama.com/install.sh | sh\\n\\n# Alternative: Homebrew\\nbrew install ollama\\n```\\n\\n### Linux\\n\\n```\\ncurl -fsSL https://ollama.com/install.sh | sh\\n```\\n\\n### Windows\\n\\n```\\n# Download installer from ollama.com\\n# Or use winget\\nwinget install Ollama.Ollama\\n```\\n\\n### Docker Installation\\n\\n```\\n# Run Ollama in Docker\\ndocker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\\n\\n# Pull and run a model\\ndocker exec -it ollama ollama run llama2\\n```\\n\\n## Starting Ollama Service\\n\\n```\\n# Start Ollama service\\nollama serve\\n\\n# Run in background (Linux/macOS)\\nnohup ollama serve > ollama.log 2>&1 &\\n```\\n\\n## Basic Model Operations\\n\\n### Pull Models\\n\\n```\\n# Pull LLaMA 2 (7B)\\nollama pull llama2\\n\\n# Pull specific model versions\\nollama pull llama2:13b\\nollama pull llama2:70b\\n\\n# Pull other popular models\\nollama pull mistral\\nollama pull codellama\\nollama pull vicuna\\nollama pull neural-chat\\n```\\n\\n### List Available Models\\n\\n```\\n# List installed models\\nollama list\\n\\n# Show model information\\nollama show llama2\\n```\\n\\n### Remove Models\\n\\n```\\n# Remove specific model\\nollama rm llama2:13b\\n\\n# Remove all versions of a model\\nollama rm llama2\\n```\\n\\n## Running Models\\n\\n### Interactive Chat\\n\\n```\\n# Start interactive session\\nollama run llama2\\n\\n# Run with specific parameters\\nollama run llama2 --temperature 0.7\\n\\n# Exit interactive mode\\n/bye\\n```\\n\\n### Single Prompt\\n\\n```\\n# One-time prompt\\nollama run llama2 \"Explain quantum computing\"\\n\\n# With custom temperature\\nollama run llama2 \"Write Python code for sorting\" --temperature 0.1\\n```\\n\\n## API Usage\\n\\n### REST API Examples\\n\\n#### Basic Chat Completion\\n\\n```\\n# Simple chat request\\ncurl http://localhost:11434/api/chat \\\\\\n -H \"Content-Type: application/json\" \\\\\\n -d \\'{\\n \"model\": \"llama2\",\\n \"messages\": [\\n {\\n \"role\": \"user\",\\n \"content\": \"Why is the sky blue?\"\\n }\\n ]\\n }\\'\\n```\\n\\n#### Streaming Response\\n\\n```\\n# Stream response\\ncurl http://localhost:11434/api/chat \\\\\\n -H \"Content-Type: application/json\" \\\\\\n -d \\'{\\n \"model\": \"llama2\",\\n \"messages\": [\\n {\\n \"role\": \"user\",\\n \"content\": \"Write a Python function\"\\n }\\n ],\\n \"stream\": true\\n }\\'\\n```\\n\\n#### Generate Text\\n\\n```\\n# Generate completion\\ncurl http://localhost:11434/api/generate \\\\\\n -H \"Content-Type: application/json\" \\\\\\n -d \\'{\\n \"model\": \"llama2\",\\n \"prompt\": \"The future of AI is\",\\n \"stream\": false\\n }\\'\\n```\\n\\n### Python Integration\\n\\n#### Basic Python Client\\n\\n```\\nimport requests\\nimport json\\n\\ndef chat_with_ollama(model, message):\\n url = \"http://localhost:11434/api/chat\"\\n data = {\\n \"model\": model,\\n \"messages\": [{\"role\": \"user\", \"content\": message}],\\n \"stream\": False\\n }\\n\\n response = requests.post(url, json=data)\\n return response.json()[\"message\"][\"content\"]\\n\\n# Usage\\nresult = chat_with_ollama(\"llama2\", \"Explain machine learning\")\\nprint(result)\\n```\\n\\n#### Streaming Python Client\\n\\n```\\nimport requests\\nimport json\\n\\ndef stream_chat(model, message):\\n url = \"http://localhost:11434/api/chat\"\\n data = {\\n \"model\": model,\\n \"messages\": [{\"role\": \"user\", \"content\": message}],\\n \"stream\": True\\n }\\n\\n with requests.post(url, json=data, stream=True) as response:\\n for line in response.iter_lines():\\n if line:\\n chunk = json.loads(line)\\n if \"message\" in chunk:\\n yield chunk[\"message\"][\"content\"]\\n\\n# Usage\\nfor token in stream_chat(\"llama2\", \"Write a story\"):\\n print(token, end=\"\", flush=True)\\n```\\n\\n#### Async Python Client\\n\\n```\\nimport aiohttp\\nimport asyncio\\nimport json\\n\\nasync def async_chat(model, message):\\n url = \"http://localhost:11434/api/chat\"\\n data = {\\n \"model\": model,\\n \"messages\": [{\"role\": \"user\", \"content\": message}],\\n \"stream\": False\\n }\\n\\n async with aiohttp.ClientSession() as session:\\n async with session.post(url, json=data) as response:\\n result = await response.json()\\n return result[\"message\"][\"content\"]\\n\\n# Usage\\nasync def main():\\n result = await async_chat(\"llama2\", \"Explain async programming\")\\n print(result)\\n\\nasyncio.run(main())\\n```\\n\\n## JavaScript/Node.js Integration\\n\\n### Basic Node.js Client\\n\\n```\\nconst axios = require(\\'axios\\');\\n\\nasync function chatWithOllama(model, message) {\\n const response = await axios.post(\\'http://localhost:11434/api/chat\\', {\\n model: model,\\n messages: [{ role: \\'user\\', content: message }],\\n stream: false\\n });\\n\\n return response.data.message.content;\\n}\\n\\n// Usage\\nchatWithOllama(\\'llama2\\', \\'Explain JavaScript promises\\')\\n .then(result => console.log(result))\\n .catch(err => console.error(err));\\n```\\n\\n### Browser Integration\\n\\n```\\n \\n \\n \\n Ollama Chat \\n \\n \\n \\n \\n Send \\n\\n \\n async function sendMessage() {\\n const input = document.getElementById(\\'input\\');\\n const message = input.value;\\n\\n try {\\n const response = await fetch(\\'http://localhost:11434/api/chat\\', {\\n method: \\'POST\\',\\n headers: {\\n \\'Content-Type\\': \\'application/json\\',\\n },\\n body: JSON.stringify({\\n model: \\'llama2\\',\\n messages: [{ role: \\'user\\', content: message }],\\n stream: false\\n })\\n });\\n\\n const data = await response.json();\\n document.getElementById(\\'chat\\').innerHTML +=\\n ` You: ${message} \\n AI: ${data.message.content} `;\\n\\n input.value = \\'\\';\\n } catch (error) {\\n console.error(\\'Error:\\', error);\\n }\\n }\\n \\n \\n \\n```\\n\\n## Custom Model Configuration\\n\\n### Create Modelfile\\n\\n```\\n# Modelfile for custom configuration\\nFROM llama2\\n\\n# Set custom parameters\\nPARAMETER temperature 0.8\\nPARAMETER top_p 0.9\\nPARAMETER top_k 40\\n\\n# Set custom system prompt\\nSYSTEM \"\"\"\\nYou are a helpful coding assistant. Always provide code examples.\\n\"\"\"\\n\\n# Set custom template\\nTEMPLATE \"\"\"{{ if .System }}<|system|>\\n{{ .System }}<|end|>\\n{{ end }}{{ if .Prompt }}<|user|>\\n{{ .Prompt }}<|end|>\\n{{ end }}<|assistant|>\\n\"\"\"\\n```\\n\\n### Build Custom Model\\n\\n```\\n# Create custom model\\nollama create my-assistant -f ./Modelfile\\n\\n# Run custom model\\nollama run my-assistant\\n```\\n\\n## Advanced Configuration\\n\\n### Environment Variables\\n\\n```\\n# Set custom host and port\\nexport OLLAMA_HOST=0.0.0.0:11434\\n\\n# Set model storage location\\nexport OLLAMA_MODELS=/custom/path/models\\n\\n# Enable debug logging\\nexport OLLAMA_DEBUG=1\\n\\n# Set maximum concurrent requests\\nexport OLLAMA_MAX_LOADED_MODELS=3\\n```\\n\\n### GPU Configuration\\n\\n```\\n# Check GPU availability\\nollama run llama2 --verbose\\n\\n# Force CPU usage\\nOLLAMA_NUM_GPU=0 ollama serve\\n\\n# Use specific GPU\\nCUDA_VISIBLE_DEVICES=0 ollama serve\\n```\\n\\n### Memory Management\\n\\n```\\n# Set context window size\\nollama run llama2 --context-length 4096\\n\\n# Set batch size for processing\\nollama run llama2 --batch-size 512\\n```\\n\\n## Popular Models and Use Cases\\n\\n### Code Generation\\n\\n```\\n# CodeLlama for programming\\nollama pull codellama\\n\\n# Use for code generation\\nollama run codellama \"Write a Python web scraper\"\\n\\n# Code completion\\nollama run codellama:code \"def fibonacci(n):\"\\n```\\n\\n### Specialized Models\\n\\n```\\n# For chat/conversation\\nollama pull vicuna\\n\\n# For reasoning tasks\\nollama pull neural-chat\\n\\n# For multilingual tasks\\nollama pull orca-mini\\n\\n# For SQL generation\\nollama pull sqlcoder\\n```\\n\\n## Troubleshooting\\n\\n### Common Issues\\n\\n#### Port Already in Use\\n\\n```\\n# Kill existing Ollama process\\npkill ollama\\n\\n# Or find and kill specific PID\\nlsof -i :11434\\nkill -9 \\n\\n# Start with different port\\nOLLAMA_HOST=0.0.0.0:11435 ollama serve\\n```\\n\\n#### Out of Memory\\n\\n```\\n# Check system resources\\nfree -h # Linux\\nvm_stat # macOS\\n\\n# Use smaller model\\nollama pull llama2:7b # Instead of 13b or 70b\\n\\n# Reduce context length\\nollama run llama2 --context-length 2048\\n```\\n\\n#### Model Download Issues\\n\\n```\\n# Clear corrupted downloads\\nrm -rf ~/.ollama/models/incomplete\\n\\n# Download with resume support\\nollama pull llama2 --insecure\\n```\\n\\n### Performance Optimization\\n\\n#### System Tuning\\n\\n```\\n# Increase file descriptors (Linux)\\nulimit -n 65536\\n\\n# Set memory limits\\nexport OLLAMA_MAX_VRAM=8GB\\n\\n# Enable memory mapping\\nexport OLLAMA_MMAP=1\\n```\\n\\n#### Monitoring\\n\\n```\\n# Check model status\\ncurl http://localhost:11434/api/tags\\n\\n# Health check\\ncurl http://localhost:11434/api/version\\n\\n# Resource usage\\ncurl http://localhost:11434/api/ps\\n```\\n\\n## Docker Compose Setup\\n\\n```\\nversion: \\'3.8\\'\\nservices:\\n ollama:\\n image: ollama/ollama\\n container_name: ollama\\n ports:\\n - \"11434:11434\"\\n volumes:\\n - ollama_data:/root/.ollama\\n environment:\\n - OLLAMA_HOST=0.0.0.0:11434\\n restart: unless-stopped\\n deploy:\\n resources:\\n reservations:\\n devices:\\n - driver: nvidia\\n count: 1\\n capabilities: [gpu]\\n\\nvolumes:\\n ollama_data:\\n```\\n\\n## Integration Examples\\n\\n### FastAPI Integration\\n\\n```\\nfrom fastapi import FastAPI\\nimport requests\\n\\napp = FastAPI()\\n\\n@app.post(\"/chat\")\\nasync def chat_endpoint(message: str, model: str = \"llama2\"):\\n response = requests.post(\"http://localhost:11434/api/chat\", json={\\n \"model\": model,\\n \"messages\": [{\"role\": \"user\", \"content\": message}],\\n \"stream\": False\\n })\\n return {\"response\": response.json()[\"message\"][\"content\"]}\\n\\n# Run with: uvicorn main:app --reload\\n```\\n\\n### Express.js Integration\\n\\n```\\nconst express = require(\\'express\\');\\nconst axios = require(\\'axios\\');\\n\\nconst app = express();\\napp.use(express.json());\\n\\napp.post(\\'/chat\\', async (req, res) => {\\n try {\\n const { message, model = \\'llama2\\' } = req.body;\\n\\n const response = await axios.post(\\'http://localhost:11434/api/chat\\', {\\n model,\\n messages: [{ role: \\'user\\', content: message }],\\n stream: false\\n });\\n\\n res.json({ response: response.data.message.content });\\n } catch (error) {\\n res.status(500).json({ error: error.message });\\n }\\n});\\n\\napp.listen(3000, () => console.log(\\'Server running on port 3000\\'));\\n```\\n\\n## CLI Automation Scripts\\n\\n### Batch Processing\\n\\n```\\n#!/bin/bash\\n# batch_process.sh\\n\\nMODEL=\"llama2\"\\nINPUT_FILE=\"prompts.txt\"\\nOUTPUT_DIR=\"outputs\"\\n\\nmkdir -p \"$OUTPUT_DIR\"\\n\\nwhile IFS= read -r prompt; do\\n echo \"Processing: $prompt\"\\n output_file=\"$OUTPUT_DIR/$(echo \"$prompt\" | sed \\'s/[^a-zA-Z0-9]/_/g\\').txt\"\\n ollama run \"$MODEL\" \"$prompt\" > \"$output_file\"\\ndone < \"$INPUT_FILE\"\\n```\\n\\n### Model Management Script\\n\\n```\\n#!/bin/bash\\n# manage_models.sh\\n\\ncase \"$1\" in\\n \"install\")\\n ollama pull llama2\\n ollama pull codellama\\n ollama pull mistral\\n ;;\\n \"cleanup\")\\n ollama list | grep -v \"NAME\" | awk \\'{print $1}\\' | xargs -I {} ollama rm {}\\n ;;\\n \"status\")\\n ollama list\\n ;;\\n *)\\n echo \"Usage: $0 {install|cleanup|status}\"\\n exit 1\\n ;;\\nesac\\n```\\n\\nThis comprehensive guide covers installation, basic usage, API integration, troubleshooting, and advanced configurations for Ollama, providing developers with practical code examples for immediate implementation.\\n\\nHave Queries? Join https://launchpass.com/collabnix\\n\\n[Tanvir Kour Follow](https://collabnix.com/author/tanvirkour/) Tanvir Kour is a passionate technical blogger and open source enthusiast. She is a graduate in Computer Science and Engineering and has 4 years of experience in providing IT solutions. She is well-versed with Linux, Docker and Cloud-Native application. You can connect to her via Twitter https://x.com/tanvirkour\\n\\n- [Ollama](https://collabnix.com/category/ollama/)\\n\\n« [10 Essential Docker Best Practices for R Developers in 2025](https://collabnix.com/10-essential-docker-best-practices-for-r-developers-in-2025/)\\n\\n[The New Ollama 0.1.0 Desktop App: Revolutionary Local AI Made Simple for Mac and Windows Users](https://collabnix.com/the-new-ollama-0-1-0-desktop-app-revolutionary-local-ai-made-simple-for-mac-and-windows-users/) »\\n\\n## [Hugging Face vs Ollama: The Complete Technical Deep Dive…](https://collabnix.com/hugging-face-vs-ollama-the-complete-technical-deep-dive-guide-for-local-ai-development-in-2025/)\\n\\n[Collabnix Team](https://collabnix.com/author/collabnixteam/)Sep 6, 20259 min read\\n\\n## [How to Fine-Tune LLM and Use It with Ollama:…](https://collabnix.com/how-to-fine-tune-llm-and-use-it-with-ollama-a-complete-guide-for-2025/)\\n\\n[Collabnix Team](https://collabnix.com/author/collabnixteam/)Aug 13, 20255 min read\\n\\n## [Types of Ollama Models: Complete Guide to Local AI…](https://collabnix.com/types-of-ollama-models-complete-guide-to-local-ai-model-varieties/)\\n\\n[Collabnix Team](https://collabnix.com/author/collabnixteam/)Aug 13, 20253 min read\\n\\n[Join our Discord Server](https://discord.gg/QEkCXAXYSe)\\n\\nTable of Contents [×](https://collabnix.com/collabnix.com)\\n\\n[→Index](https://collabnix.com/collabnix.com)', title='Complete Ollama Guide: Installation, Usage & Code Examples', url='https://collabnix.com/complete-ollama-guide-installation-usage-code-examples')]\n"
     ]
    }
   ],
   "source": [
    "# from dotenv import load_dotenv                                                  # type: ignore\n",
    "# load_dotenv()\n",
    "\n",
    "import ollama                                                                   # type: ignore\n",
    "response = ollama.web_search(\"What is Ollama?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking:  Okay, the user is asking about Ollama's new engine. Let me think. First, I need to figure out what Ollama is. Ollama is an open-source language model, right? It's developed by Ollama, a company. The user is probably looking for information on their latest updates or features.\n",
      "\n",
      "The tools available are web_search and web_fetch. Since the user is asking about a new engine, I should use web_search to find recent information. The parameters for web_search include query and max_results. The query here would be \"ollama new engine\" and max_results could be 3 to get the most relevant results. I need to make sure to format the tool_call correctly with the function name and arguments in JSON. Let me check if there are any other parameters required, but the function only needs query and max_results. Alright, that should work.\n",
      "\n",
      "Tool calls:  [ToolCall(function=Function(name='web_search', arguments={'max_results': 3, 'query': 'ollama new engine'}))]\n",
      "Result:  results=[WebSearchResult(content=\"# Ollama's new engine for multimodal models\\n\\n## May 15, 2025\\n\\nOllama now supports multimodal models via Ollama’s new engine, starting with new vision multimodal m...\n",
      "Thinking:  Okay, the user is asking about Ollama's new engine. I need to provide a clear and concise answer based on the web search results. Let me start by summarizing the key points from the search results.\n",
      "\n",
      "First, the new engine supports multimodal models like vision, text, and more. The results mention specific models such as Meta Llama 4, Google Gemma 3, and Qwen 2.5 VL. These are all examples of the new engine's capabilities. \n",
      "\n",
      "Next, I should highlight the main features of the new engine. The user might be interested in how the engine improves model reliability and accuracy. The response mentions model modularity, which is important for separation of text and vision models. Also, there's a focus on accuracy and memory management, which are crucial for handling image processing efficiently.\n",
      "\n",
      "I need to make sure the answer is structured clearly, perhaps dividing the new engine's features into model capabilities, improvements, and future directions. The user might also be interested in the impact of this on the Ollama platform. I should check if there's any additional information I'm missing, but from the provided results, the main points are covered.\n",
      "\n",
      "Content:  Ollama's new engine supports multimodal models like vision, text, and more. Key features include:\n",
      "\n",
      "1. **Model Modularity**: Separation of text and vision models for better reliability and simplification of integration.\n",
      "2. **Accuracy**: Improves handling of large image tokens and positional information to enhance output quality.\n",
      "3. **Memory Management**: Optimizes image caching and KV cache optimizations for efficient local inference.\n",
      "\n",
      "These improvements make Ollama a robust platform for multimodal tasks.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# from dotenv import load_dotenv                                                  # type: ignore\n",
    "# load_dotenv()\n",
    "\n",
    "from ollama import chat, web_fetch, web_search                                  # type: ignore\n",
    "\n",
    "\n",
    "available_tools = {'web_search': web_search, 'web_fetch': web_fetch}\n",
    "\n",
    "messages = [{'role': 'user', 'content': \"what is ollama's new engine\"}]\n",
    "\n",
    "while True:\n",
    "\n",
    "    MODEL = 'qwen3:0.6b' #'qwen3:4b'\n",
    "\n",
    "    response = chat(model=MODEL, messages=messages, tools=[web_search, web_fetch], think=True)\n",
    "\n",
    "    if response.message.thinking: print('Thinking: ', response.message.thinking)\n",
    "\n",
    "    if response.message.content: print('Content: ', response.message.content)\n",
    "    \n",
    "    messages.append(response.message)\n",
    "    \n",
    "    if not response.message.tool_calls: break\n",
    "\n",
    "    print('Tool calls: ', response.message.tool_calls)\n",
    "    \n",
    "    for tool_call in response.message.tool_calls: function_to_call = available_tools.get(tool_call.function.name)\n",
    "\n",
    "    if function_to_call:\n",
    "        args = tool_call.function.arguments\n",
    "        result = function_to_call(**args)\n",
    "        print('Result: ', str(result)[:200]+'...')\n",
    "        # Result is truncated for limited context lengths\n",
    "        messages.append({'role': 'tool', 'content': str(result)[:2000 * 4], 'tool_name': tool_call.function.name})\n",
    "    else:\n",
    "        messages.append({'role': 'tool', 'content': f'Tool {tool_call.function.name} not found', 'tool_name': tool_call.function.name})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama-websearch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
